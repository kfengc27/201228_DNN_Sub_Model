{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import regularizers\n",
    "import tensorboard\n",
    "tensorboard.__version__\n",
    "from sklearn.metrics import r2_score\n",
    "import pydot\n",
    "from tensorflow.keras.layers import Activation\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read DV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dvs \n",
    "DVs = pd.read_csv('./Data/Input/DVs.csv')\n",
    "\n",
    "# BIG 5\n",
    "extraversion = DVs['extraversion']\n",
    "agreeableness = DVs['agreeableness']\n",
    "conscientiousness = DVs['conscientiousness']\n",
    "neuroticism = DVs['neuroticism']\n",
    "openness = DVs['openness']\n",
    "\n",
    "# Other Dvs \n",
    "implusiveness = DVs['implusiveness']\n",
    "rational = DVs['rational']\n",
    "experiential = DVs['experiential']\n",
    "normative_influence = DVs['normative.influence']\n",
    "informational_influence = DVs['informational.influence']\n",
    "\n",
    "# Potential DVs\n",
    "need_for_closure = DVs['need_for_closure']\n",
    "utilitarian = DVs['reason.utilitarian']\n",
    "hedonic = DVs['reason.hedonic']\n",
    "self_inflation = DVs['self_inflation']\n",
    "individual = DVs['individual']\n",
    "\n",
    "collective = DVs['collective']\n",
    "\n",
    "# create a list to store DVs\n",
    "DVs_list = [extraversion, agreeableness, conscientiousness, neuroticism, openness, implusiveness, rational, experiential,normative_influence, informational_influence, need_for_closure, utilitarian, hedonic, individual,collective ]\n",
    "DVs_list_name = [\"extraversion\", \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\", \"implusiveness\", \"rational\", \"experiential\",\"normative_influence\", \"informational_influence\", \"need_for_closure\", \"utilitarian\", \"hedonic\", \"individual\",\"collective\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read IV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_IVs_without_reasons_to_use_FB = pd.read_csv('./Data/Input_content/All_IVs_without_reasons_to_use_FB.csv')\n",
    "All_IVs_without_reasons_to_use_FB_plus_demo = pd.read_csv('./Data/Input_content/All_IVs_without_reasons_to_use_FB_plus_demo.csv')\n",
    "\n",
    "All_IVs_without_reasons_to_use_FB_And_activity = pd.read_csv(\"./Data/Input_content/All_IVs_without_reasons_to_use_FB_And_activity.csv\")\n",
    "All_IVs_without_reasons_to_use_FB_And_activity_plus_demo = pd.read_csv(\"./Data/Input_content/All_IVs_without_reasons_to_use_FB_And_activity_plus_demo.csv\")\n",
    "\n",
    "All_IVs_without_reasons_to_use_FB_And_content = pd.read_csv('./Data/Input_content/All_IVs_without_reasons_to_use_FB_And_content.csv')\n",
    "All_IVs_without_reasons_to_use_FB_And_content_plus_demo = pd.read_csv('./Data/Input_content/All_IVs_without_reasons_to_use_FB_And_content_plus_demo.csv')\n",
    "\n",
    "# create a list to store 10 input IVs \n",
    "IVs_list = [All_IVs_without_reasons_to_use_FB, All_IVs_without_reasons_to_use_FB_plus_demo, All_IVs_without_reasons_to_use_FB_And_activity,All_IVs_without_reasons_to_use_FB_And_activity_plus_demo, All_IVs_without_reasons_to_use_FB_And_content,All_IVs_without_reasons_to_use_FB_And_content_plus_demo]\n",
    "\n",
    "IVs_list_name = [\"All_IVs_without_reasons_to_use_FB\", \"All_IVs_without_reasons_to_use_FB_plus_demo\", \"All_IVs_without_reasons_to_use_FB_And_activity\",\"All_IVs_without_reasons_to_use_FB_And_activity_plus_demo\", \"All_IVs_without_reasons_to_use_FB_And_content\",\"All_IVs_without_reasons_to_use_FB_And_content_plus_demo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(All_IVs_without_reasons_to_use_FB.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_content = pd.read_csv('./Data/Input_content/fb_content.csv')\n",
    "len(fb_content.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DNN Model with a sub model \n",
    "def build_and_compile_dnn_model(input_shape):\n",
    "#     other_variables_num = len(All_IVs_without_reasons_to_use_FB.columns) # assume the number of variables is 67 here. In the real program, the number of input variables will be calcuated automatically. \n",
    "    # sub model \n",
    "    fb_content_input = keras.Input(shape=(21,), name='fb_content')\n",
    "    fb_content_hidden_layer = keras.layers.Dense(10, activation='relu')(fb_content_input)\n",
    "    fb_content_output = keras.layers.Dense(5, name='sub_output_layer', activation='linear')(fb_content_hidden_layer)\n",
    "    # other part of model \n",
    "    fb_others_input = keras.Input(shape=(input_shape,))\n",
    "    fb_concatted_input = tf.keras.layers.Concatenate()([fb_content_output, fb_others_input]) # concat two types of input together \n",
    "    fb_concatted_hidden_layer1 = keras.layers.Dense(20, input_dim=2, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4), activation=tf.nn.relu)(fb_concatted_input)\n",
    "    fb_concatted_hidden_1_dropout = keras.layers.Dropout(.2, name='hidden1_Dropout')(fb_concatted_hidden_layer1)\n",
    "    fb_concatted_hidden_layer2 = keras.layers.Dense(20, input_dim=2, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.l2(1e-4), activation=tf.nn.relu)(fb_concatted_hidden_1_dropout)\n",
    "    fb_concatted_hidden_2_dropout = keras.layers.Dropout(.2, name='hidden2_Dropout')(fb_concatted_hidden_layer2)\n",
    "    fb_concatted_output = keras.layers.Dense(1, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(fb_concatted_hidden_2_dropout)\n",
    "    model = tf.keras.Model(inputs=[fb_content_input, fb_others_input], outputs=fb_concatted_output)\n",
    "    learn_rate = 0.001\n",
    "    opt = keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average score from a list \n",
    "def average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_r2(Input_IVs,Input_IVs_name, Input_DVs,DVs_list_name,filename, fb_content):\n",
    "    IVs = Input_IVs \n",
    "    DVs_list = Input_DVs \n",
    "    IVs_list_name = Input_IVs_name\n",
    "    r2_ave_result = pd.DataFrame(columns = DVs_list_name)\n",
    "    r2_max_result = pd.DataFrame(columns = DVs_list_name)\n",
    "    m = 0\n",
    "    all_r2_ave_list = []\n",
    "    all_r2_max_list = []\n",
    "    input_val = [fb_content, IVs]\n",
    "    i = 0 \n",
    "    for DV in DVs_list: \n",
    "            # run 20 simulations\n",
    "        r2_list = [] \n",
    "        for k in range(0, 1):\n",
    "            dnn_model = build_and_compile_dnn_model(len(IVs.columns))\n",
    "            dnn_model.fit(input_val, DV, validation_split=0.2, verbose=0, epochs=2000)\n",
    "            r2_list.append(r2_score(DV, dnn_model.predict(input_val)))\n",
    "        print(\"R2 is :\", average(r2_list))\n",
    "        sub_model = tf.keras.Model(dnn_model.get_layer(index = 0).output, dnn_model.get_layer(index = 2).output)\n",
    "        inp = sub_model.predict(fb_content)\n",
    "        sub_layer = tf.keras.layers.Softmax()\n",
    "        sub_layer_result = pd.DataFrame(sub_layer(inp).numpy())\n",
    "        sub_layer_file_name = IVs_list_name + \"_predict_by_linear_\" + DVs_list_name[i] + '.csv'\n",
    "        sub_layer_result.to_csv(sub_layer_file_name)\n",
    "        all_r2_ave_list.append(average(r2_list))\n",
    "        all_r2_max_list.append(max(r2_list))\n",
    "        m = m + 1 \n",
    "        i = i + 1 \n",
    "    all_r2_ave_list_series = pd.Series(all_r2_ave_list, index = r2_ave_result.columns)\n",
    "    all_r2_max_list_series = pd.Series(all_r2_max_list, index = r2_max_result.columns)\n",
    "    r2_ave_result = r2_ave_result.append(all_r2_ave_list_series, ignore_index = True)\n",
    "    r2_max_result = r2_max_result.append(all_r2_max_list_series, ignore_index = True)\n",
    "    today = time.strftime(\"%Y-%m-%d\")\n",
    "    ave_file = today + \"linear\" + filename + '.csv'\n",
    "    max_file = today + \"linear\" + filename + '.csv'\n",
    "    r2_ave_result.to_csv(ave_file)\n",
    "    r2_max_result.to_csv(max_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(num,IVs_list, IVs_list_name,DVs_list,DVs_list_name, fb_content):\n",
    "    cal_r2(IVs_list[num],IVs_list_name[num], DVs_list,DVs_list_name,IVs_list_name[num], fb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,6):\n",
    "#     worker(i, IVs_list, IVs_list_name,DVs_list,DVs_list_name, fb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is : 0.46675895244834786\n",
      "R2 is : 0.6265568070419166\n",
      "R2 is : 0.5434915188424396\n",
      "R2 is : 0.5393136240469634\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "for i in range(0, 4):\n",
    "    t = threading.Thread(target=worker, args=(i,IVs_list, IVs_list_name,DVs_list,DVs_list_name, fb_content))\n",
    "    threads.append(t)\n",
    "    t.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
